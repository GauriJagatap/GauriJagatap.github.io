<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://www.public.iastate.edu/~gauri/feed.xml" rel="self" type="application/atom+xml" /><link href="http://www.public.iastate.edu/~gauri/" rel="alternate" type="text/html" /><updated>2017-07-23T00:02:16+05:30</updated><id>http://www.public.iastate.edu/~gauri/</id><title type="html">Gauri Jagatap</title><subtitle>A blog about technology and stuff related</subtitle><entry><title type="html">Topic extraction using NMF and Sparse PCA</title><link href="http://www.public.iastate.edu/~gauri/topic-extraction-using-NMF-and-Sparse-PCA/" rel="alternate" type="text/html" title="Topic extraction using NMF and Sparse PCA" /><published>2017-07-11T19:48:00+05:30</published><updated>2017-07-11T19:48:00+05:30</updated><id>http://www.public.iastate.edu/~gauri/topic-extraction-using-NMF-and-Sparse-PCA</id><content type="html" xml:base="http://www.public.iastate.edu/~gauri/topic-extraction-using-NMF-and-Sparse-PCA/">&lt;p&gt;A summary of two methods for text analysis, excerpts of my project and linking the code (GitHub).&lt;/p&gt;

&lt;p&gt;[to be updated]&lt;/p&gt;</content><author><name></name></author><category term="project" /><category term="machine learning" /><category term="data science" /><summary type="html">A summary of two methods for text analysis, excerpts of my project and linking the code (GitHub).</summary></entry><entry><title type="html">ReLU-based neural networks</title><link href="http://www.public.iastate.edu/~gauri/ReLU-based-neural-net/" rel="alternate" type="text/html" title="ReLU-based neural networks" /><published>2017-06-16T19:48:00+05:30</published><updated>2017-06-16T19:48:00+05:30</updated><id>http://www.public.iastate.edu/~gauri/ReLU-based-neural-net</id><content type="html" xml:base="http://www.public.iastate.edu/~gauri/ReLU-based-neural-net/">&lt;p&gt;Learning ReLUs using gradient descent: an insight.&lt;/p&gt;

&lt;p&gt;Close on the heals of the phase retrieval problem is that of establishing performance guarantees on single layer neural networks with ReLU activation functions.&lt;/p&gt;

&lt;p&gt;[to be updated]&lt;/p&gt;</content><author><name></name></author><category term="blog" /><category term="machine learning" /><category term="data science" /><summary type="html">Learning ReLUs using gradient descent: an insight.</summary></entry><entry><title type="html">How interpretable is data?</title><link href="http://www.public.iastate.edu/~gauri/how-interpretable-is-data/" rel="alternate" type="text/html" title="How interpretable is data?" /><published>2017-05-05T19:48:00+05:30</published><updated>2017-05-05T19:48:00+05:30</updated><id>http://www.public.iastate.edu/~gauri/how-interpretable-is-data</id><content type="html" xml:base="http://www.public.iastate.edu/~gauri/how-interpretable-is-data/">&lt;p&gt;I am finally done with my second semester towards my PhD, which means it’s time for sum-mer and some-more (or a-lot-more) research!&lt;/p&gt;

&lt;p&gt;I happened to have two course projects that I only recently wrapped up, and they turned out to be somewhat related! The two topics being sparse principal component analysis (SPCA) and non-negative matrix factorization (NMF). Both of them, key tools to help &lt;em&gt;interpret&lt;/em&gt; data better.&lt;/p&gt;

&lt;p&gt;So wait. Given a set of data points, can’t we as humans do the intelligible task of interpretation? What do these data-interpretations tools do that we can’t?&lt;/p&gt;

&lt;p&gt;The answer: they don’t do anything we can’t. They are just better at interpreting a &lt;em&gt;larger scale&lt;/em&gt; of data. They’re like a self-organizing library. The librarian no longer has to assign books to particular sections, the books do that themselves (not that we want to put librarians out of business)!&lt;/p&gt;

&lt;p&gt;Those familiar with machine learning will automatically recognize this problem formulation as that of &lt;em&gt;unsupervised learning&lt;/em&gt;. Employ algorithms that make sense out of data! Principal component analysis, does just that. It tries to represent the variation in the data in descending order. The first principal direction has the maximum variation in data. Usually the first few principal components (usually, this number is $\leq r$, where $r$ is &lt;em&gt;rank&lt;/em&gt; of the data matrix) are sufficient to explain most of the (variation in) data. Now these “directions” are composed of the relative “importance” of its constituent features.&lt;/p&gt;

&lt;p&gt;Mathematically speaking, the PCA problem boils down to the singular value decomposition,&lt;/p&gt;

&lt;center&gt;$ M_{d \times n} = US_{d \times r} V^T_{r \times n} $&lt;/center&gt;

&lt;p&gt;where our data matrix $M$ is assumed to lie in a lower dimensional subspace of rank $r$. Sparse PCA, additionally assumes that the right singular vectors, which are columns of $V$ are &lt;em&gt;sparse&lt;/em&gt; .&lt;/p&gt;

&lt;p&gt;The non-negative matrix factorization problem is similar. A non-negative matrix can be decomposed into non-negative matrices $W,H$,&lt;/p&gt;

&lt;center&gt;$ M_{d \times n} = W_{d \times r} H_{r \times n} $&lt;/center&gt;

&lt;p&gt;The basic concept utilized in both of these methods is the same: most data has an underlying structure. Imposing the knowledge of this structure should help us extract meaningful information about this data.&lt;/p&gt;

&lt;p&gt;Like what? For example in a text dataset, most articles focus on a few core topics. Further, these core topics, can be represented using few core words. This spurred several cool applications, such as detection of &lt;em&gt;trends&lt;/em&gt; on social media. In image processing, this has useful applications in segmentation. Representing images as a sum or weighted sum of components. Demixing of audio signals. The list goes on and on and I bet you can already sense the theme in this one.&lt;/p&gt;</content><author><name></name></author><category term="blog" /><category term="machine learning" /><category term="data science" /><summary type="html">I am finally done with my second semester towards my PhD, which means it’s time for sum-mer and some-more (or a-lot-more) research!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://www.public.iastate.edu/~gauri/assets/images/machinelearning.jpg" /></entry><entry><title type="html">Phase retrieval of structured signals</title><link href="http://www.public.iastate.edu/~gauri/phase-retrieval-of-structured-signals/" rel="alternate" type="text/html" title="Phase retrieval of structured signals" /><published>2017-03-12T19:48:00+05:30</published><updated>2017-03-12T19:48:00+05:30</updated><id>http://www.public.iastate.edu/~gauri/phase-retrieval-of-structured-signals</id><content type="html" xml:base="http://www.public.iastate.edu/~gauri/phase-retrieval-of-structured-signals/">&lt;p&gt;We consider the problem of recovering a signal $\mathbf{x}^* \in \mathbb{R}^n$, from magnitude-only measurements, $y_i = | \left\langle {\mathbf{a}_i},{\mathbf{x}^*} \right\rangle | $ for $i={1,2,\ldots,m}$. This is a stylized version of the classical phase retrieval problem, and is a fundamental challenge in nano- and bio-imaging systems, astronomical imaging, and speech processing. It is well known that the above problem is ill-posed, and therefore some additional assumptions on the signal and/or the measurements are necessary.&lt;/p&gt;

&lt;p&gt;In this paper, we first study the case where the underlying signal $\mathbf{x}^*$ is $s$-sparse. For this case, we develop a novel recovery algorithm that we call &lt;em&gt;Compressive Phase Retrieval with Alternating Minimization&lt;/em&gt;, or &lt;em&gt;CoPRAM&lt;/em&gt;. Our algorithm is simple and be obtained via a natural combination of the classical alternating minimization approach for phase retrieval with the CoSaMP algorithm for sparse recovery. Despite its simplicity, we prove that our algorithm achieves a sample complexity of $O(s^2 \log n)$ with Gaussian measurements $\mathbf{a}_i$, which matches the best known existing results; moreover, it also demonstrates linear convergence in theory and practice. An appealing feature of our algorithm is that it requires no extra tuning parameters other than the signal sparsity level $s$.&lt;/p&gt;

&lt;p&gt;We then consider the case where the underlying signal $\mathbf{x}^*$ arises from &lt;em&gt;structured&lt;/em&gt; sparsity models. We specifically examine the case of &lt;em&gt;block-sparse&lt;/em&gt; signals with uniform block size of $b$ and block sparsity $k=s/b$. For this problem, we design a recovery algorithm that we call &lt;em&gt;Block CoPRAM&lt;/em&gt; that further reduces the sample complexity to $O(ks \log n)$. For sufficiently large block lengths of $b=\Theta(s)$, this bound equates to $O(s \log n)$. To our knowledge, this constitutes the first end-to-end linearly convergent algorithm for phase retrieval where the Gaussian sample complexity has a sub-quadratic dependence on the signal sparsity level.&lt;/p&gt;

&lt;p&gt;You can read the full paper on &lt;a target=&quot;_blank&quot; href=&quot;https://arxiv.org/abs/1705.06412&quot;&gt; arXiv &lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can also find all codes related to the paper on my &lt;a target=&quot;_blank&quot; href=&quot;https://github.com/GauriJagatap/model-copram&quot;&gt; GitHub page&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="project" /><category term="machine learning" /><category term="data science" /><summary type="html">We consider the problem of recovering a signal $\mathbf{x}^* \in \mathbb{R}^n$, from magnitude-only measurements, $y_i = | \left\langle {\mathbf{a}_i},{\mathbf{x}^*} \right\rangle | $ for $i={1,2,\ldots,m}$. This is a stylized version of the classical phase retrieval problem, and is a fundamental challenge in nano- and bio-imaging systems, astronomical imaging, and speech processing. It is well known that the above problem is ill-posed, and therefore some additional assumptions on the signal and/or the measurements are necessary.</summary></entry><entry><title type="html">Recovery of structured sparse signals using model CoSaMP</title><link href="http://www.public.iastate.edu/~gauri/recovery-of-structured-sparse-signals-using-Model-CoSaMP/" rel="alternate" type="text/html" title="Recovery of structured sparse signals using model CoSaMP" /><published>2017-01-18T19:48:00+05:30</published><updated>2017-01-18T19:48:00+05:30</updated><id>http://www.public.iastate.edu/~gauri/recovery-of-structured-sparse-signals-using-Model-CoSaMP</id><content type="html" xml:base="http://www.public.iastate.edu/~gauri/recovery-of-structured-sparse-signals-using-Model-CoSaMP/">&lt;p&gt;A survey of structured sparse signal recovery algorithms with emphasis on Model CoSaMP.&lt;/p&gt;

&lt;p&gt;[to be updated]&lt;/p&gt;</content><author><name></name></author><category term="blog" /><category term="machine learning" /><category term="data science" /><summary type="html">A survey of structured sparse signal recovery algorithms with emphasis on Model CoSaMP.</summary></entry><entry><title type="html">Phase retrieval of sparse signals</title><link href="http://www.public.iastate.edu/~gauri/phase-retrieval-of-sparse-signals/" rel="alternate" type="text/html" title="Phase retrieval of sparse signals" /><published>2016-09-23T19:48:00+05:30</published><updated>2016-09-23T19:48:00+05:30</updated><id>http://www.public.iastate.edu/~gauri/phase-retrieval-of-sparse-signals</id><content type="html" xml:base="http://www.public.iastate.edu/~gauri/phase-retrieval-of-sparse-signals/">&lt;p&gt;A survey of sparse phase retrieval algos, followed by an introduction to CoPRAM.&lt;/p&gt;

&lt;p&gt;[to be updated]&lt;/p&gt;</content><author><name></name></author><category term="project" /><category term="machine learning" /><category term="data science" /><summary type="html">A survey of sparse phase retrieval algos, followed by an introduction to CoPRAM.</summary></entry><entry><title type="html">Compressed sensing for ultrasound imaging</title><link href="http://www.public.iastate.edu/~gauri/compressed-sensing-for-ultrasound-imaging/" rel="alternate" type="text/html" title="Compressed sensing for ultrasound imaging" /><published>2016-07-15T19:48:00+05:30</published><updated>2016-07-15T19:48:00+05:30</updated><id>http://www.public.iastate.edu/~gauri/compressed-sensing-for-ultrasound-imaging</id><content type="html" xml:base="http://www.public.iastate.edu/~gauri/compressed-sensing-for-ultrasound-imaging/">&lt;p&gt;My introduction to data science and the field of compressed sensing was primarily through the project that I did at the Spectrum Lab of the Indian Institute of Science. It involved analyzing ultrasound images and their underlying sparsity and using the compressed sensing framework to reduce the number of samples acquired in the imaging process. While most ultrasound images are sparse in the standard basis itself, I considered a number of wavelet bases that could emulate the impulse response of the ultrasound transducer, which promotes further sparsity.&lt;/p&gt;

&lt;p&gt;It essentially boils down to solving the well known Lasso problem, which I did, using Alternating Directions Method of Multipliers (ADMM).&lt;/p&gt;

&lt;p&gt;Lasso Problem.&lt;/p&gt;

&lt;p&gt;Link to basic code (on GitHub).&lt;/p&gt;

&lt;p&gt;[to be updated]&lt;/p&gt;</content><author><name></name></author><category term="project" /><category term="compressed sensing" /><category term="ultrasound imaging" /><summary type="html">My introduction to data science and the field of compressed sensing was primarily through the project that I did at the Spectrum Lab of the Indian Institute of Science. It involved analyzing ultrasound images and their underlying sparsity and using the compressed sensing framework to reduce the number of samples acquired in the imaging process. While most ultrasound images are sparse in the standard basis itself, I considered a number of wavelet bases that could emulate the impulse response of the ultrasound transducer, which promotes further sparsity.</summary></entry><entry><title type="html">Phase retrieval using alternating minimization</title><link href="http://www.public.iastate.edu/~gauri/phase-retrieval-using-alternating-minimization/" rel="alternate" type="text/html" title="Phase retrieval using alternating minimization" /><published>2016-07-03T19:48:00+05:30</published><updated>2016-07-03T19:48:00+05:30</updated><id>http://www.public.iastate.edu/~gauri/phase-retrieval-using-alternating-minimization</id><content type="html" xml:base="http://www.public.iastate.edu/~gauri/phase-retrieval-using-alternating-minimization/">&lt;p&gt;Netrapalli’s paper. Correlation with Wirtinger Flow. How do Fourier measurements come into play.&lt;/p&gt;

&lt;p&gt;[to be updated]&lt;/p&gt;</content><author><name></name></author><category term="blog" /><category term="machine learning" /><category term="data science" /><summary type="html">Netrapalli’s paper. Correlation with Wirtinger Flow. How do Fourier measurements come into play.</summary></entry><entry><title type="html">Compressed sensing using matching pursuit</title><link href="http://www.public.iastate.edu/~gauri/compressed-sensing-using-CoSaMP/" rel="alternate" type="text/html" title="Compressed sensing using matching pursuit" /><published>2016-06-28T19:48:00+05:30</published><updated>2016-06-28T19:48:00+05:30</updated><id>http://www.public.iastate.edu/~gauri/compressed-sensing-using-CoSaMP</id><content type="html" xml:base="http://www.public.iastate.edu/~gauri/compressed-sensing-using-CoSaMP/">&lt;p&gt;Compressed Sensing Using Matching Pursuit or CoSaMP, happens to be a standard technique to solve compressed sensing problems.&lt;/p&gt;

&lt;p&gt;Algorithm.&lt;/p&gt;

&lt;p&gt;Subspace Pursuit.&lt;/p&gt;

&lt;p&gt;Code.&lt;/p&gt;

&lt;p&gt;[to be updated]&lt;/p&gt;</content><author><name></name></author><category term="blog" /><category term="compressed sensing" /><category term="machine learning" /><category term="data science" /><summary type="html">Compressed Sensing Using Matching Pursuit or CoSaMP, happens to be a standard technique to solve compressed sensing problems.</summary></entry><entry><title type="html">Video segmentation using ADMM</title><link href="http://www.public.iastate.edu/~gauri/video-segmentation/" rel="alternate" type="text/html" title="Video segmentation using ADMM" /><published>2015-02-10T19:48:00+05:30</published><updated>2015-02-10T19:48:00+05:30</updated><id>http://www.public.iastate.edu/~gauri/video-segmentation</id><content type="html" xml:base="http://www.public.iastate.edu/~gauri/video-segmentation/">&lt;p&gt;This was my starter kit to the world of data science. I did this project in a largely unguided fashion, but it gave me a good idea about low-rank and sparse problems in signal processing.&lt;/p&gt;

&lt;p&gt;An excerpt:&lt;/p&gt;

&lt;p&gt;Video data transmitted by surveillance cameras is generally processed to detect
moving objects automatically. The video generally consists of a moving object that covers a small fraction of a video frame and majority of the frame is spanned by the background. If each frame is vectorized, and these vectors are concatenated, it is referred to this as the video volume. The video volume can be split into the background and the moving objects (background separation). An intuitive method to separate out the background is by using the fact that the background being stationary, will form the low rank part of the video volume. On the other hand, the moving objects constitute the sparse component. This decomposition is done using a low rank and sparse decomposition of the video volume.&lt;/p&gt;

&lt;p&gt;The speed of this processing however, is slowed down by the abundance of data
collected, which mostly consists of spells of inactivity. Compressive sensing is a technique used to acquire video data in a different basis, instead of the usual spatial basis, like Fourier or Wavelet, that involves acquiring a small fraction (up to 50% in this paper, lower fractions can be used for larger number of frames or higher resolution data) of the data that would have been acquired in the spatial basis.&lt;/p&gt;

&lt;p&gt;This method works when the given data is sparse in this basis, which validates the low sampling rate (less than Nyquist). Sparse signal recovery from such compressive measurements is a process of minimizing the nuclear norm or l0 norm of the signal (in this case, video volume) in the transformed basis (which ensures sparsity in the transformed basis). The whole video volume can be recovered from this norm minimization and further background separation techniques can be employed to separate out the moving objects.&lt;/p&gt;

&lt;p&gt;However, one can formulate the minimization problem such that the background
and moving objects are separated out during the recovery. Moreover, one
1
can choose tight wavelet transforms, which have specific properties that help simplify the minimization problem significantly.&lt;/p&gt;

&lt;p&gt;The implementation described is based on &lt;a target=&quot;_blank&quot; href=&quot;https://arxiv.org/abs/1302.1942&quot;&gt; this paper &lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can find details of my project work &lt;a href=&quot;http://www.public.iastate.edu/~gauri/assets/vid_seg.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="blog" /><category term="machine learning" /><category term="data science" /><summary type="html">This was my starter kit to the world of data science. I did this project in a largely unguided fashion, but it gave me a good idea about low-rank and sparse problems in signal processing.</summary></entry></feed>